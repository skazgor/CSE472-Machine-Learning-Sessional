# -*- coding: utf-8 -*-
"""EM+GMM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16m4aoVYNFiM6xLSgWTxtSOUcdFfMfaRq
"""

import numpy as np
import matplotlib.pyplot as plt

import sys

from scipy.stats import multivariate_normal

# take file name and size of k from argument
file_name = sys.argv[1]
K = int(sys.argv[2])

if K < 2:
    print("K should be greater than 1")
    sys.exit()

data = np.genfromtxt(file_name, delimiter = ',')


# convergence threshold
threshold = 1e-6

if data.shape[1] > 2:

    mean_vals = np.mean(data, axis=0)
    origin_data = data - mean_vals

    U, S, Vt = np.linalg.svd(data, full_matrices=False)

    principal_axis = Vt[:2, :]

    projected_data = np.dot(origin_data, principal_axis.T)

    data = projected_data

plt.scatter(data[:, 0], data[:, 1], marker='o', color='b', label='Data Points')

plt.xlabel('Axis 1')
plt.ylabel('Axis 2')
plt.title('2 Dimention data')

plt.legend()

plt.savefig('data_before_cluster.png')

# plt.show()

plt.close()

# K = 4

NUM_ITERATION = 100

def plot_gmm(k, data, means, covariances, weights):

    probabilities = np.zeros((k,len(data)))
    for i in range(k):
        probabilities[i, :] = weights[i] * multivariate_normal.pdf(data, means[i], covariances[i])

    n_components = len(means)

    # Scatter plot of the data points
    for i in range(n_components):
        component_data = data[np.argmax(probabilities, axis=0) == i]
        plt.scatter(component_data[:, 0], component_data[:, 1], alpha=0.7, label=f'Component {i+1}')

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(f'Estimated GMM with K={len(means)}')
    plt.legend()
    plt.savefig(f'estimated_gmm_plot_for_K_{k}.png')
    plt.show()

def get_likelihood(data, means, covariances, weights):
    
    likelihoods = np.zeros(len(data))

    for i in range(len(means)):
        likelihoods += weights[i] * multivariate_normal.pdf(data, means[i], covariances[i])

    current_likelihood = np.sum(np.log(likelihoods))
    
    return current_likelihood

for k in range(2, K+1):

    best_likelihood = None
    best_parameters = None

    for i in range(5):

        # initialization
        n, m = data.shape
        means = data[np.random.choice(n, k, replace=False)]

        covariances = [np.eye(m) for _ in range(k)]

        weights = np.ones(k) / k
        
        prev_likelihood = 0.0

        for j in range(NUM_ITERATION):

            # Expectation step
            probabilities = np.zeros((k, len(data)))

            for i in range(k):
                probabilities[i, :] = weights[i] * multivariate_normal.pdf(data, means[i], covariances[i])

            probabilities /= probabilities.sum(axis=0, keepdims=True)

            # Maximization step
            means = np.dot(probabilities, data) / probabilities.sum(axis=1, keepdims=True)

            for i in range(k):
                diff = data - means[i]
                covariances[i] = np.dot(probabilities[i, :].T * diff.T, diff) / probabilities[i, :].sum()

            weights = probabilities.sum(axis=1) / n
            
            current_likelihood = np.sum(np.log(np.sum(probabilities, axis=1)))
            
            if np.abs(current_likelihood - prev_likelihood) < threshold:
                break

        
        current_likelihood = get_likelihood(data, means, covariances, weights)

        if best_likelihood is None or current_likelihood > best_likelihood:
            best_likelihood = current_likelihood
            best_parameters = (means, covariances, weights)
    plot_gmm(k,data, best_parameters[0], best_parameters[1], best_parameters[2])

