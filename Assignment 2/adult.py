# -*- coding: utf-8 -*-
"""Adult.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KqszAnVRorMTjMG9a6PJ39zvnqMGu80c
"""

!curl -O https://archive.ics.uci.edu/static/public/2/adult.zip

!unzip adult.zip

# !pip install scikit-learn==1.3.2

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.feature_selection import SelectKBest, mutual_info_classif

from sklearn.metrics import  confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder
# from sklearn.preprocessing import TargetEncoder,
# Make NumPy printouts easier to read.
np.set_printoptions(precision=5, suppress=True)

header_names = [
    'age',
    'workclass',
    'fnlwgt',
    'education',
    'education-num',
    'marital-status',
    'occupation',
    'relationship',
    'race',
    'sex',
    'capital-gain',
    'capital-loss',
    'hours-per-week',
    'native-country',
    'income'
]

train_df=pd.read_csv("adult.data",header=None,names=header_names)
train_df.head()

test_df=pd.read_csv("adult.test",header=None,names=header_names,skiprows=1)
test_df.head()

train_df.info()

test_df.info()

columns=train_df.select_dtypes(exclude=['object']).columns

# normalization
for col in columns:
  mean_value = train_df[col].mean()
  std_value = train_df[col].std()

  train_df[col] = (train_df[col] - mean_value) / std_value
  test_df[col] = (test_df[col]-mean_value)/std_value
train_df.sample(1)

def plot_count(df):
  categorical_columns = df.select_dtypes(include=['object']).columns

  fig, axes = plt.subplots(nrows=len(categorical_columns), ncols=1, figsize=(10, 4 * len(categorical_columns)))

  for i, column in enumerate(categorical_columns):
      sns.countplot(x=column, data=df, ax=axes[i])
      axes[i].set_title(f"Count of {column}")

  plt.tight_layout()
  plt.show()

plot_count(train_df)

categorical_columns_with_missing = ['workclass', 'occupation', 'native-country']
for column in categorical_columns_with_missing:
  non_missing_values = train_df[train_df[column] != ' ?'][column]
  probabilities = non_missing_values.value_counts(normalize=True)

  print(train_df[column].unique())
  missing_values_count_train = (train_df[column] == ' ?').sum()
  missing_values_count_test = (test_df[column] == ' ?').sum()

  print(missing_values_count_train)

  random_sample_train = np.random.choice(probabilities.index, size=missing_values_count_train, p=probabilities.values)
  random_sample_test = np.random.choice(probabilities.index, size=missing_values_count_test, p=probabilities.values)

  train_df.loc[train_df[column] == ' ?', column] = random_sample_train
  test_df.loc[test_df[column] == ' ?', column] = random_sample_test

plot_count(train_df)

train_df.drop(columns=['education'], inplace=True)
test_df.drop(columns=['education'], inplace=True)
test_df.sample(1)

def level_encode_for_binary_class(df):
  categorical_columns = df.select_dtypes(include=['object']).columns

  # Create a new DataFrame to store encoded columns
  df_encoded = df.copy()

  for col in categorical_columns:
    if df[col].nunique() == 2:
      possible_labels = df[col].unique()

      label_dict = {}
      for index, possible_label in enumerate(possible_labels):
        label_dict[possible_label] = index
      print(label_dict)
      df_encoded[col] = df[col].replace(label_dict)
  return df_encoded

train_df=level_encode_for_binary_class(train_df)
test_df=level_encode_for_binary_class(test_df)

others=.1
df=train_df.copy()
df_test=test_df.copy()

categorical_columns = df.select_dtypes(include=['object']).columns

threshold_count = len(df) * (1-others)

for column in categorical_columns:
  top_categories = df[column].value_counts()
  num_categories_to_include = (top_categories.cumsum() <= threshold_count).sum()+1

  encoder = OneHotEncoder(max_categories=num_categories_to_include +1,sparse=False)

  encoded_categories = encoder.fit_transform(df[[column]])
  encoded_categories_test = encoder.fit_transform(df_test[[column]])

  encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out([column]))
  encoded_df_test=pd.DataFrame(encoded_categories_test, columns=encoder.get_feature_names_out([column]))

  df=pd.concat([df,encoded_df],axis=1)
  df.drop(columns=[column], inplace=True)

  df_test=pd.concat([df_test,encoded_df_test],axis=1)
  df_test.drop(columns=[column],inplace=True)

print(df.sample(1))
df_test.sample(1)

x_train=df.drop('income',axis=1)
y_train=df['income']

x_test=df_test.drop('income',axis=1)
y_test=df_test['income']

N=30

selector = SelectKBest(score_func=mutual_info_classif, k='all')
selector.fit(x_train, y_train)

feature_scores = pd.DataFrame({'Feature': x_train.columns, 'Information_Gain': selector.scores_})
feature_scores = feature_scores.sort_values(by='Information_Gain', ascending=False)
selected_features = feature_scores.head(N)['Feature'].tolist()


x_train_subset = x_train[selected_features]
x_test_subset = x_test[selected_features]

x_train_subset.head()

y_train.head()

train_x=x_train_subset.values
train_y = y_train.values

test_x = x_test_subset.values
test_y = y_test.values

train_y=train_y.reshape(-1, 1)
test_y=test_y.reshape(-1,1)

test_x.shape

def sigmoid(z):
  return 1 / (1 + np.exp(-z))

def initialize_weights(num_features):
  return np.zeros((num_features, 1))

def add_intercept_column(X):
  intercept = np.ones((X.shape[0], 1))
  return np.concatenate((intercept, X), axis=1)

def accuracy_with_intercept(X,Y, weights):
  predictions = sigmoid(np.dot(X, weights))
  y_pred=(predictions >= 0.5).astype(int)
  accuracy_ = np.mean(predictions == Y)
  return accuracy_

def logistic_regression(X, y, learning_rate=0.01, num_iterations=5000, error_rate=.5):
  X = add_intercept_column(X)
  m, n = X.shape
  weights = initialize_weights(n)

  for i in range(num_iterations):
    if 1-accuracy_with_intercept(X,y,weights)<error_rate:
      return weights
    z = np.dot(X, weights)
    predictions = sigmoid(z)
    error = predictions - y
    gradient = np.dot(X.T, error) / m
    weights -= learning_rate * gradient

  return weights

print(logistic_regression(train_x,train_y,error_rate=0.0))

def AdaBoost(X,y,l_weak,K,seed=42,error_rate=.5):
  np.random.seed(seed)

  intercepted_X=add_intercept_column(X)
  N,temp=X.shape

  w = np.ones(N) / N
  w = w.reshape(-1, 1)

  h=[]
  weights=np.zeros(K)

  for k in range(K):
    samples_index=np.random.choice(N, size=N, replace=True, p=w.flatten())

    samples_x=X[samples_index]
    samples_y= y[samples_index]

    h_k = l_weak(samples_x,samples_y)

    z = np.dot(intercepted_X, h_k)
    predictions_probability=sigmoid(z)
    predictions_lebel=(predictions_probability >= 0.5).astype(int)
    print((predictions_lebel!=y).astype(int).shape)

    error=np.sum(np.dot(w.T,predictions_lebel!=y))

    print(error)

    if error> .5:
      continue

    for i in range(N):
      if predictions_lebel[i] == y[i]:
        w[i]=w[i]*error/(1-error)
    w=w/np.sum(w)

    h.append(h_k)
    weights[k]=np.log((1 - error) / max(error, 1e-10))
  return h,weights

def weighted_majority(h, z, x):
  votes=np.zeros((x.shape[0],1))

  print(votes.shape)
  x=add_intercept_column(x)
  for h_z, z_k in zip(h,z):

    z_values=np.dot(x,h_z)
    predictions_probability=sigmoid(z_values)
    predictions_lebel=(predictions_probability >= 0.5).astype(int)

    predictions_lebel[predictions_lebel == 0] = -1

    print(predictions_lebel.shape)
    votes+=predictions_lebel
  return (votes>0 ).astype(int)

h,z=AdaBoost(train_x,train_y,logistic_regression,20, seed=42)

y_pred=weighted_majority(h,z,test_x)

conf_matrix = confusion_matrix(test_y.flatten(), y_pred.flatten())
TN, FP, FN, TP = conf_matrix.ravel()

# Accuracy
accuracy = (TP + TN) / (TP + TN + FP + FN)

# True Positive Rate (Recall)
recall = TP / (TP + FN)

# True Negative Rate (Specificity)
specificity = TN / (TN + FP)

# Positive Predictive Value (Precision)
precision = TP / (TP + FP)

# False Discovery Rate
fdr = FP / (TP + FP)

# F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"Recall (True Positive Rate): {recall:.4f}")
print(f"Specificity (True Negative Rate): {specificity:.4f}")
print(f"Precision (Positive Predictive Value): {precision:.4f}")
print(f"False Discovery Rate: {fdr:.4f}")
print(f"F1 Score: {f1_score:.4f}")

